{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries and Dataset"
      ],
      "metadata": {
        "id": "mbTzZn4yBHu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
        "from torchmetrics.functional.classification import auroc\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import math"
      ],
      "metadata": {
        "id": "L6f-hCIr3fxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f983ce-ea00-412e-a3f5-94899a2ec82f"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "articles_path = '/content/drive/MyDrive/combined_data.csv'\n",
        "labeled_articles = pd.read_csv(articles_path)"
      ],
      "metadata": {
        "id": "gZ7gyBaU5X-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA and Data Pre-Processing"
      ],
      "metadata": {
        "id": "S32A6akjBVf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## gut check - should be about 37k articles and 3 columns\n",
        "\n",
        "labeled_articles = labeled_articles[['title', 'content_original', 'bias']]\n",
        "\n",
        "print(f\"shape: {labeled_articles.shape}\")\n",
        "print(f\"columns: {list(labeled_articles.columns)}\")\n",
        "\n",
        "## relevant columns: source, bias, content, title\n",
        "\n",
        "# labeled_articles_head = labeled_articles.head()\n",
        "# labeled_articles_head.to_csv(\"head.csv\", index=False)\n",
        "\n",
        "print(f\"counts:\\n\\n{labeled_articles['bias'].value_counts()}\\n\")\n",
        "print(f\"percentages:\\n\\n{labeled_articles['bias'].value_counts(normalize=True)}\\n\")\n",
        "\n",
        "\n",
        "# base model: always predict conservative - most common class: should have 36.5% accuracy (easy to beat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKqmvcXB4ah1",
        "outputId": "28a7af74-4a53-4aa5-b87f-32d591a741a3"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (37554, 3)\n",
            "columns: ['title', 'content_original', 'bias']\n",
            "counts:\n",
            "\n",
            "2    13734\n",
            "0    13005\n",
            "1    10815\n",
            "Name: bias, dtype: int64\n",
            "\n",
            "percentages:\n",
            "\n",
            "2    0.365713\n",
            "0    0.346301\n",
            "1    0.287985\n",
            "Name: bias, dtype: float64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Big problem: RoBERTa can only take in so many tokens and the average political article is way too big for the encoder. This is where we could do more pre-processing (i.e. breaking down the article to its \"essential components)."
      ],
      "metadata": {
        "id": "_xtN7PA2CWVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "also, this is where i would split the articles into training, validation, and test"
      ],
      "metadata": {
        "id": "FDl-0m1WGU6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "1jA5DIu0M4iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArticleDataSet(Dataset):\n",
        "  def __init__(self, article_df, label_df, tokenizer, max_token_len: int = 128):\n",
        "    '''\n",
        "    takes feature dataframe (article text + title) and the label dataframe (numeric political bias)\n",
        "    training, validation, test splits must be done BEFORE this step\n",
        "    '''\n",
        "\n",
        "    self.article_df = article_df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.label_df = label_df\n",
        "    self.max_token_len = max_token_len\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.article_df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = self.article_df.iloc[index]\n",
        "    label = self.label_df.iloc[index]\n",
        "\n",
        "    text_content = str(item.content_original)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ## vectorize / encode\n",
        "    tokens = self.tokenizer.encode_plus(text_content,\n",
        "                                        add_special_tokens=True,\n",
        "                                        return_tensors='pt',\n",
        "                                        truncation=True,\n",
        "                                        max_length=self.max_token_len,\n",
        "                                        padding='max_length',\n",
        "                                        return_attention_mask=True)\n",
        "\n",
        "    return {'input_ids': tokens.input_ids.flatten(),\n",
        "            'attention_mask': tokens.attention_mask.flatten(),\n",
        "            'label': label}\n"
      ],
      "metadata": {
        "id": "tlIwAq6SNHfV"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "features = labeled_articles[['content_original']]\n",
        "labels = labeled_articles[['bias']]\n",
        "\n",
        "train_ds = ArticleDataSet(features, labels, tokenizer)\n",
        "## gut check\n",
        "## train_ds.__getitem__(0)"
      ],
      "metadata": {
        "id": "930q-w0MQjKS"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKnWT8p_pbNC",
        "outputId": "a8635424-d747-40f2-b8cc-8c7d4c030c42"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37554"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Module\n",
        "\n",
        "The purpose of a PyTorch data module is to separate data-related code from model-related code"
      ],
      "metadata": {
        "id": "Pven-eRUbW3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Article_Data_Module(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, train_ds, labels, val_ds=None, test_ds=None, batch_size: int = 16, max_token_len=128, model_name = 'roberta-base'):\n",
        "    '''\n",
        "    train_ds, val_ds, labels should all be data frames\n",
        "    '''\n",
        "    super().__init__()\n",
        "\n",
        "    self.train_ds = train_ds\n",
        "    self.val_ds = val_ds\n",
        "    self.test_ds = test_ds\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    self.max_token_len = max_token_len\n",
        "    self.model_name = model_name\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    if stage in (None, \"fit\"):\n",
        "      self.train_dataset = ArticleDataSet(self.train_ds, self.tokenizer, self.labels)\n",
        "      if self.val_ds is not None:\n",
        "        self.val_dataset = ArticleDataSet(self.val_ds, self.tokenizer, self.labels)\n",
        "    if stage in (None, \"predict\", \"test\"):\n",
        "      self.test_dataset = ArticleDataSet(self.test_ds, self.tokenizer, self.labels)\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_dataset, batch_size = self.batch_size, num_workers=4, shuffle=True)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_dataset, batch_size = self.batch_size, num_workers=4, shuffle=False)\n",
        "\n",
        "  def predict_dataloader(self):\n",
        "    return DataLoader(self.val_dataset, batch_size = self.batch_size, num_workers=4, shuffle=False)"
      ],
      "metadata": {
        "id": "UDGSuBeCdCVc"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_dm = Article_Data_Module(features, labels)"
      ],
      "metadata": {
        "id": "W7Va3KrtgZi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_dm.setup()"
      ],
      "metadata": {
        "id": "qlbG0yT2ljBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_dl = article_dm.train_dataloader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zll6gBBLpDmp",
        "outputId": "c49f91c1-93bc-4d02-80c9-cdbc323eceb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(article_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr6MVXvOpJsO",
        "outputId": "1c96a33b-48a6-478f-9817-2b5a16c87e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2348"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "shIHHd-Zpo0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Article_Classifier(pl.LightningModule):\n",
        "  def __init__(self, config: dict):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n",
        "\n",
        "    ## add in hidden layer and final layer\n",
        "\n",
        "    self.hidden = nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
        "    self.classifier = nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels'])\n",
        "\n",
        "    torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.hidden.weight)\n",
        "    self.loss_func = nn.CrossEntropyLoss()\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, label=None):\n",
        "    '''\n",
        "    label is None during prediction\n",
        "    '''\n",
        "    print(f\"label: {label}\")\n",
        "\n",
        "    # roberta model\n",
        "    output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pooled_output = torch.mean(output.last_hidden_state, 1)\n",
        "\n",
        "    # neural network classification layers\n",
        "\n",
        "    pooled_output = self.hidden(pooled_output)\n",
        "    pooled_output = self.dropout(pooled_output)\n",
        "    pooled_output = F.relu(pooled_output)\n",
        "\n",
        "    logits = self.classifier(pooled_output)\n",
        "\n",
        "\n",
        "    loss = 0\n",
        "    if label is not None:\n",
        "      loss = self.loss_func(logits, label)\n",
        "      return loss, logits\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def training_step(self, batch, batch_index):\n",
        "    loss, logits = self(**batch)  # unpack - will call forward pass\n",
        "    self.log(\"train loss\", loss, prog_bar = True, logger = True)\n",
        "    return {\"loss\": loss, \"predictions\": logits, \"labels\": batch['labels']}\n",
        "\n",
        "  def validation_step(self, batch, batch_index):\n",
        "    loss, logits = self(**batch)  # unpack - will call forward pass\n",
        "    self.log(\"validation loss\", loss, prog_bar = True, logger = True)\n",
        "    return {\"val_loss\": loss, \"predictions\": logits, \"labels\": batch['labels']}\n",
        "\n",
        "  def predict_step(self, batch, batch_index):\n",
        "    _, logits = self(**batch)  # unpack - will call forward pass\n",
        "    return logits\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['w_decay'])\n",
        "    total_steps = self.config['train_size'] / self.config['batch_size']\n",
        "    warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "    return [optimizer], [scheduler]\n",
        "\n"
      ],
      "metadata": {
        "id": "CWv4_WSLw7JH"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# could just use regular roberta\n",
        "config = {\n",
        "  'model_name' : 'distilroberta-base',\n",
        "  'n_labels': 3,\n",
        "  'batch_size': 128,\n",
        "  'lr': 1.5e-6,\n",
        "  'warmup': 0.2,\n",
        "  'train_size': len(article_dl),\n",
        "  'w_decay': 0.001,\n",
        "  'n_epochs': 100\n",
        "}\n",
        "\n",
        "model = Article_Classifier(config)"
      ],
      "metadata": {
        "id": "-Yc_yyqR23BB"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "input_ids = train_ds.__getitem__(idx)['input_ids']\n",
        "am = train_ds.__getitem__(idx)['attention_mask']\n",
        "lbl_value = train_ds.__getitem__(idx)['label'].item()\n",
        "lbl_tensor = torch.tensor([lbl_value], dtype=torch.long)\n",
        "#print(f\"label: {lbl_value}\")\n",
        "\n",
        "loss, output = model(input_ids.unsqueeze(dim=0), am.unsqueeze(dim=0), lbl_tensor)  # need to fix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1wNffjW5kxh",
        "outputId": "a577064e-90c4-43a7-9dc8-07c18d71c9de"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label: tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "nyPb6NXC_bM9",
        "outputId": "9b1c6a42-df58-4508-9fdd-fff3243c3327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7043, grad_fn=<NllLossBackward0>)\n",
            "tensor([[1.1278, 0.4321, 0.4809]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GlTPRgJ6_pce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "xOYtPtLD5nbg"
      }
    }
  ]
}